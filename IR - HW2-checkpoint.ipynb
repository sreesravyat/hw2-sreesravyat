{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3890b92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import string\n",
    "from collections import OrderedDict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "495f5f99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thi', 'is', 'an', 'exampl', 'sentenc', '.', 'note', 'how', 'nltk', 'token', 'and', 'stem', 'each', 'word', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jagad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Download the necessary NLTK models and corpora\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Correct import statements for word_tokenize and PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Example usage\n",
    "text = \"This is an example sentence. Note how NLTK tokenizes and stems each word.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Initialize the PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stem each word in the tokenized text\n",
    "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "print(stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a68cf2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(text, ps):\n",
    "    words = word_tokenize(text)\n",
    "    stemmed_words = [ps.stem(word) for word in words]\n",
    "    stemmed_text = ' '.join(stemmed_words)\n",
    "    return stemmed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3afaaf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_map = {}\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def parse_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "        #print(content)\n",
    "        \n",
    "    doc_entries = content.split('<DOC>')[1:]\n",
    "    for each_doc in doc_entries:\n",
    "        doc_start = each_doc.find('<DOCNO>') + len('<DOCNO>')\n",
    "        doc_end = each_doc.find('</DOCNO>', doc_start)\n",
    "        doc_no = each_doc[doc_start:doc_end].strip()\n",
    "            \n",
    "        text_start = each_doc.find('<TEXT>') + len('<TEXT>')\n",
    "        text_end = each_doc.find('</TEXT>', text_start)\n",
    "        text = each_doc[text_start:text_end].strip()\n",
    "             \n",
    "        stemmed_text = stem_text(text, ps)\n",
    "        \n",
    "        if doc_no in text_map:\n",
    "            text_map[doc_no] += '\\n' + stemmed_text\n",
    "        else:\n",
    "            text_map[doc_no] = stemmed_text\n",
    "    return stemmed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94b2ca41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done parsing\n"
     ]
    }
   ],
   "source": [
    "folder = \"C:/Users/jagad/Downloads/IR_data/IR_data/AP_DATA/ap89_collection\"\n",
    "\n",
    "for filename in os.listdir(folder):\n",
    "    if filename != 'readme':\n",
    "        file_path = folder + '/' + filename\n",
    "        parse_file(file_path)\n",
    "        \n",
    "print('Done parsing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcaf8678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 20, 1), (2, 20, 2), (3, 20, 3), (4, 20, 4), (1, 20, 5), (2, 20, 6), (5, 20, 7)]\n",
      "{'the': 1, 'car': 2, 'was': 3, 'in': 4, 'wash': 5}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Regular expression for the token: letters and numbers, possibly separated by single periods\n",
    "token_pattern = re.compile(r'\\b(\\w+(?:\\.\\w+)*)\\b')\n",
    "\n",
    "# Function to tokenize the text according to the given pattern\n",
    "def tokenize(text):\n",
    "    return [token.lower() for token in token_pattern.findall(text)]\n",
    "\n",
    "# Function to assign unique IDs to tokens and documents\n",
    "def assign_ids(tokens):\n",
    "    term_id_map = {}\n",
    "    term_id_counter = 1  # Start term IDs at 1\n",
    "    \n",
    "    # Assign a unique ID to each term\n",
    "    for token in tokens:\n",
    "        if token not in term_id_map:\n",
    "            term_id_map[token] = term_id_counter\n",
    "            term_id_counter += 1\n",
    "            \n",
    "    return term_id_map\n",
    "\n",
    "# Function to convert a document to a sequence of (term_id, doc_id, position) tuples\n",
    "def document_to_tuples(doc_id, text):\n",
    "    tokens = tokenize(text)\n",
    "    term_id_map = assign_ids(tokens)\n",
    "    doc_tuples = []\n",
    "    \n",
    "    # Create (term_id, doc_id, position) tuples\n",
    "    for position, token in enumerate(tokens):\n",
    "        term_id = term_id_map[token]\n",
    "        doc_tuples.append((term_id, doc_id, position + 1))  # Position starts at 1\n",
    "    \n",
    "    return doc_tuples, term_id_map\n",
    "\n",
    "# Example usage:\n",
    "doc_text = \"The car was in the car wash.\"\n",
    "doc_id = 20\n",
    "doc_tuples, term_id_map = document_to_tuples(doc_id, doc_text)\n",
    "\n",
    "# Output the tuples and the term ID map\n",
    "print(doc_tuples)\n",
    "print(term_id_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61e3d832",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, namedtuple\n",
    "import re\n",
    "import gzip\n",
    "import pickle\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Namedtuples for clarity\n",
    "Posting = namedtuple('Posting', ['doc_id', 'tf', 'positions'])\n",
    "InvertedList = namedtuple('InvertedList', ['df', 'ttf', 'postings'])\n",
    "\n",
    "# Inverted Index Structure\n",
    "inverted_index = defaultdict(lambda: InvertedList(df=0, ttf=0, postings=[]))\n",
    "\n",
    "# Auxiliary structures\n",
    "doc_ids = {}  # Map from doc names to doc ids\n",
    "term_ids = {}  # Map from terms to term ids\n",
    "next_doc_id = 1\n",
    "next_term_id = 1\n",
    "total_terms = 0  # Vocabulary size\n",
    "total_tokens = 0  # Total number of tokens\n",
    "\n",
    "# Add nltk downloads for stopwords and tokenizer if not already available\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "586f5ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, remove_stopwords=False, stem=False):\n",
    "    # Tokenize by whitespace and remove punctuation\n",
    "    tokens = re.findall(r'\\b\\w+(?:\\.\\w+)*\\b', text.lower())\n",
    "    \n",
    "    # Remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Stem tokens\n",
    "    if stem:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce639b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_document_to_index(doc_name, text):\n",
    "    global next_doc_id, next_term_id, total_terms, total_tokens\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = tokenize(text, remove_stopwords=True, stem=True)\n",
    "    doc_id = doc_ids.setdefault(doc_name, next_doc_id)\n",
    "    next_doc_id += 1\n",
    "    \n",
    "    # Build postings list\n",
    "    term_positions = defaultdict(list)\n",
    "    for position, term in enumerate(tokens, start=1):\n",
    "        term_positions[term].append(position)\n",
    "    \n",
    "    # Update the inverted index\n",
    "    for term, positions in term_positions.items():\n",
    "        term_id = term_ids.setdefault(term, next_term_id)\n",
    "        next_term_id += 1\n",
    "        \n",
    "        tf = len(positions)\n",
    "        total_tokens += tf\n",
    "        \n",
    "        # Retrieve the inverted list and update it\n",
    "        inverted_list = inverted_index[term]\n",
    "        inverted_list.df += 1\n",
    "        inverted_list.ttf += tf\n",
    "        inverted_list.postings.append(Posting(doc_id, tf, positions))\n",
    "        \n",
    "    total_terms = len(term_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dca71514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_compressed_index(index, filename):\n",
    "    with gzip.open(filename, 'wb') as file:\n",
    "        pickle.dump(dict(index), file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ed7fc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "\n",
    "# Assuming nltk's data has been downloaded\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "class InvertedIndex:\n",
    "    def __init__(self):\n",
    "        self.index = defaultdict(list)  # term -> list of postings\n",
    "        self.doc_lengths = defaultdict(int)  # doc_id -> doc length\n",
    "        self.term_id_map = {}  # term -> term_id\n",
    "        self.doc_id_map = {}  # doc_name -> doc_id\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stemmer = PorterStemmer()\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        tokens = re.findall(r'\\b\\w+(?:\\.\\w+)*\\b', text.lower())\n",
    "        tokens = [token for token in tokens if token not in self.stop_words]\n",
    "        tokens = [self.stemmer.stem(token) for token in tokens]\n",
    "        return tokens\n",
    "    \n",
    "    def add_document(self, doc_name, text):\n",
    "        doc_id = self.doc_id_map.setdefault(doc_name, len(self.doc_id_map) + 1)\n",
    "        tokens = self.tokenize(text)\n",
    "        self.doc_lengths[doc_id] = len(tokens)\n",
    "        \n",
    "        term_positions = defaultdict(list)\n",
    "        for position, term in enumerate(tokens):\n",
    "            term_id = self.term_id_map.setdefault(term, len(self.term_id_map) + 1)\n",
    "            term_positions[term_id].append(position + 1)\n",
    "        \n",
    "        for term_id, positions in term_positions.items():\n",
    "            posting = {'doc_id': doc_id, 'tf': len(positions), 'positions': positions}\n",
    "            self.index[term_id].append(posting)\n",
    "    \n",
    "    def save_index(self, filepath):\n",
    "        with open(filepath, 'w') as file:\n",
    "            for term_id, postings_list in sorted(self.index.items(), key=lambda x: x[0]):\n",
    "                term = [term for term, id_ in self.term_id_map.items() if id_ == term_id][0]\n",
    "                postings_str = '|'.join([f\"{posting['doc_id']},{posting['tf']},{' '.join(map(str, posting['positions']))}\" for posting in postings_list])\n",
    "                file.write(f\"{term}:{postings_str}\\n\")\n",
    "\n",
    "# Example usage\n",
    "indexer = InvertedIndex()\n",
    "\n",
    "# Assuming you have a function to get document names and their contents\n",
    "def get_documents():\n",
    "    # This should be replaced with the actual logic to read documents\n",
    "    # For example, reading files from a directory\n",
    "    return [('doc1', \"Example text for doc1.\"), ('doc2', \"Text for the second document.\")]\n",
    "\n",
    "for doc_name, text in get_documents():\n",
    "    indexer.add_document(doc_name, text)\n",
    "\n",
    "# Save the index to a file\n",
    "indexer.save_index('inverted_index.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "440470a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inverted_index': defaultdict(<class 'list'>, {0: [(0, 2, [1, 2]), (1, 1, [2]), (2, 1, [2])], 1: [(0, 1, [3])], 2: [(1, 1, [1])], 3: [(1, 1, [3])], 4: [(2, 1, [1])], 5: [(2, 1, [3])]}), 'term_id_map': {'car': 0, 'wash': 1, 'bob': 2, 'red': 3, 'alic': 4, 'blue': 5}, 'doc_id_map': {0: 'The car was in the car wash.', 1: \"Bob's car is red.\", 2: \"Alice's car is blue.\"}, 'df_cf': {0: (3, 4), 1: (1, 1), 2: (1, 1), 3: (1, 1), 4: (1, 1), 5: (1, 1)}, 'total_vocabulary_size': 6, 'total_tokens': 8}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "class Indexer:\n",
    "    def __init__(self):\n",
    "        self.term_id_map = {}\n",
    "        self.doc_id_map = {}\n",
    "        self.term_counter = 0\n",
    "        self.doc_counter = 0\n",
    "        self.inverted_index = defaultdict(list)\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def tokenize(self, document):\n",
    "        tokens = re.findall(r'\\b\\w+(?:\\.\\w+)*\\b', document.lower())\n",
    "        return tokens\n",
    "\n",
    "    def remove_stopwords(self, tokens):\n",
    "        return [token for token in tokens if token not in self.stop_words]\n",
    "\n",
    "    def stem_tokens(self, tokens):\n",
    "        return [self.stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    def index_document(self, document):\n",
    "        doc_id = self.doc_counter\n",
    "        self.doc_id_map[doc_id] = document\n",
    "        tokens = self.tokenize(document)\n",
    "        tokens = self.remove_stopwords(tokens)\n",
    "        tokens = self.stem_tokens(tokens)\n",
    "        token_positions = defaultdict(list)\n",
    "\n",
    "        for position, token in enumerate(tokens, start=1):\n",
    "            term_id = self.term_id_map.setdefault(token, self.term_counter)\n",
    "            if term_id == self.term_counter:\n",
    "                self.term_counter += 1\n",
    "            token_positions[term_id].append(position)\n",
    "\n",
    "        for term_id, positions in token_positions.items():\n",
    "            self.inverted_index[term_id].append((doc_id, len(positions), positions))\n",
    "\n",
    "        self.doc_counter += 1\n",
    "\n",
    "    def build_index(self, documents):\n",
    "        for document in documents:\n",
    "            self.index_document(document)\n",
    "\n",
    "        # Calculate DF and CF\n",
    "        df_cf = {term_id: (len(postings), sum(tf for _, tf, _ in postings)) for term_id, postings in self.inverted_index.items()}\n",
    "\n",
    "        # Calculate total vocabulary size and total number of tokens\n",
    "        total_vocabulary_size = len(self.term_id_map)\n",
    "        total_tokens = sum(len(postings) for postings in self.inverted_index.values())\n",
    "\n",
    "        return {\n",
    "            'inverted_index': self.inverted_index,\n",
    "            'term_id_map': self.term_id_map,\n",
    "            'doc_id_map': self.doc_id_map,\n",
    "            'df_cf': df_cf,\n",
    "            'total_vocabulary_size': total_vocabulary_size,\n",
    "            'total_tokens': total_tokens\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "indexer = Indexer()\n",
    "documents = [\n",
    "    \"The car was in the car wash.\",\n",
    "    \"Bob's car is red.\",\n",
    "    \"Alice's car is blue.\"\n",
    "]\n",
    "index = indexer.build_index(documents)\n",
    "print(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9965274e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "class AP89Indexer:\n",
    "    def __init__(self):\n",
    "        self.term_id_map = {}\n",
    "        self.doc_id_map = {}\n",
    "        self.term_counter = 0\n",
    "        self.doc_counter = 0\n",
    "        self.inverted_index = defaultdict(list)\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def load_documents(self, directory):\n",
    "        documents = []\n",
    "        for filename in os.listdir(directory):\n",
    "            with open(os.path.join(directory, filename), 'r') as file:\n",
    "                documents.append(file.read())\n",
    "        return documents\n",
    "\n",
    "    def tokenize(self, document):\n",
    "        tokens = re.findall(r'\\b\\w+(?:\\.\\w+)*\\b', document.lower())\n",
    "        return tokens\n",
    "\n",
    "    def remove_stopwords(self, tokens):\n",
    "        return [token for token in tokens if token not in self.stop_words]\n",
    "\n",
    "    def stem_tokens(self, tokens):\n",
    "        return [self.stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    def index_document(self, document):\n",
    "        doc_id = self.doc_counter\n",
    "        self.doc_id_map[doc_id] = document\n",
    "        tokens = self.tokenize(document)\n",
    "        tokens = self.remove_stopwords(tokens)\n",
    "        tokens = self.stem_tokens(tokens)\n",
    "        token_positions = defaultdict(list)\n",
    "\n",
    "        for position, token in enumerate(tokens, start=1):\n",
    "            term_id = self.term_id_map.setdefault(token, self.term_counter)\n",
    "            if term_id == self.term_counter:\n",
    "                self.term_counter += 1\n",
    "            token_positions[term_id].append(position)\n",
    "\n",
    "        for term_id, positions in token_positions.items():\n",
    "            self.inverted_index[term_id].append((doc_id, len(positions), positions))\n",
    "\n",
    "        self.doc_counter += 1\n",
    "\n",
    "    def build_index(self, directory):\n",
    "        documents = self.load_documents(directory)\n",
    "        for document in documents:\n",
    "            self.index_document(document)\n",
    "\n",
    "        # Calculate DF and CF\n",
    "        df_cf = {term_id: (len(postings), sum(tf for _, tf, _ in postings)) for term_id, postings in self.inverted_index.items()}\n",
    "\n",
    "        # Calculate total vocabulary size and total number of tokens\n",
    "        total_vocabulary_size = len(self.term_id_map)\n",
    "        total_tokens = sum(len(postings) for postings in self.inverted_index.values())\n",
    "\n",
    "        return {\n",
    "            'inverted_index': self.inverted_index,\n",
    "            'term_id_map': self.term_id_map,\n",
    "            'doc_id_map': self.doc_id_map,\n",
    "            'df_cf': df_cf,\n",
    "            'total_vocabulary_size': total_vocabulary_size,\n",
    "            'total_tokens': total_tokens\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "indexer = AP89Indexer()\n",
    "index = indexer.build_index(\"C:/Users/jagad/Downloads/IR_data/IR_data/AP_DATA/ap89_collection\")\n",
    "print(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "decf2a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch7 import Elasticsearch\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "print(es.ping())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c6ea8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index\n",
    "index_name = \"inverted_index\"\n",
    "\n",
    "configurations = {\n",
    "    \"settings\" : {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 1\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"content\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fielddata\": True,\n",
    "                \"index_options\": \"positions\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ccd8085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jagad\\AppData\\Local\\Temp\\ipykernel_21656\\3782372786.py:1: DeprecationWarning: The 'body' parameter is deprecated for the 'create' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  es.indices.create(index = index_name, body = configurations)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'inverted_index'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.create(index = index_name, body = configurations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9393789",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'decodestring' from 'base64' (C:\\Users\\jagad\\conda3\\Lib\\base64.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01melasticsearch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Elasticsearch\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTokenizer\u001b[39;00m:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "File \u001b[1;32m~\\conda3\\Lib\\site-packages\\elasticsearch\\__init__.py:24\u001b[0m\n\u001b[0;32m     21\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melasticsearch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m logger\u001b[38;5;241m.\u001b[39maddHandler(logging\u001b[38;5;241m.\u001b[39mNullHandler())\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Elasticsearch\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransport\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Transport\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection_pool\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConnectionPool, ConnectionSelector, RoundRobinSelector\n",
      "File \u001b[1;32m~\\conda3\\Lib\\site-packages\\elasticsearch\\client\\__init__.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unicode_literals\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransport\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Transport\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransportError\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m string_types, urlparse, unquote\n",
      "File \u001b[1;32m~\\conda3\\Lib\\site-packages\\elasticsearch\\transport.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m chain\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m python_version\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Urllib3HttpConnection\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection_pool\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConnectionPool, DummyConnectionPool\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserializer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JSONSerializer, Deserializer, DEFAULT_SERIALIZERS\n",
      "File \u001b[1;32m~\\conda3\\Lib\\site-packages\\elasticsearch\\connection\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Connection\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhttp_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RequestsHttpConnection\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhttp_urllib3\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Urllib3HttpConnection, create_ssl_context\n",
      "File \u001b[1;32m~\\conda3\\Lib\\site-packages\\elasticsearch\\connection\\http_requests.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbase64\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decodestring\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'decodestring' from 'base64' (C:\\Users\\jagad\\conda3\\Lib\\base64.py)"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.token_regex = re.compile(r'\\b(?:[a-zA-Z0-9]+(?:\\.[a-zA-Z0-9]+)*)\\b')\n",
    "\n",
    "    def tokenize(self, document):\n",
    "        tokens = self.token_regex.findall(document.lower())\n",
    "        return tokens\n",
    "\n",
    "class Indexer:\n",
    "    def __init__(self, es_client):\n",
    "        self.es = es_client\n",
    "        self.term_id_map = {}\n",
    "        self.doc_id_map = {}\n",
    "        self.total_tokens = 0\n",
    "\n",
    "    def index_documents(self, documents):\n",
    "        inverted_index = defaultdict(list)\n",
    "\n",
    "        for doc_id, document in documents.items():\n",
    "            self.doc_id_map[doc_id] = len(self.doc_id_map)\n",
    "\n",
    "            tokens = Tokenizer().tokenize(document)\n",
    "            self.total_tokens += len(tokens)\n",
    "\n",
    "            for position, term in enumerate(tokens, start=1):\n",
    "                term_id = self.term_id_map.setdefault(term, len(self.term_id_map))\n",
    "                inverted_index[term_id].append((self.doc_id_map[doc_id], position))\n",
    "\n",
    "        index_data = []\n",
    "        for term_id, postings in inverted_index.items():\n",
    "            index_data.append({\n",
    "                \"term_id\": term_id,\n",
    "                \"postings\": postings\n",
    "            })\n",
    "\n",
    "        self.es.indices.create(index='inverted_index', ignore=400)\n",
    "        self.es.index(index='inverted_index', body={\"terms\": index_data})\n",
    "\n",
    "class SearchEngine:\n",
    "    def __init__(self, es_client):\n",
    "        self.es = es_client\n",
    "\n",
    "    def search(self, query):\n",
    "        res = self.es.search(index='inverted_index', body={\"query\": {\"match\": {\"text\": query}}})\n",
    "        return res['hits']['hits']\n",
    "\n",
    "# Sample AP89 data dictionary\n",
    "ap89_data = {\n",
    "    \"doc1\": \"The car was in the car wash.\",\n",
    "    \"doc2\": \"I am using Elasticsearch for indexing and searching documents.\"\n",
    "}\n",
    "\n",
    "# Connect to Elasticsearch running in Docker\n",
    "es = Elasticsearch(['http://localhost:9200'])\n",
    "\n",
    "# Index documents\n",
    "indexer = Indexer(es)\n",
    "indexer.index_documents(ap89_data)\n",
    "\n",
    "# Search for a query\n",
    "search_engine = SearchEngine(es)\n",
    "results = search_engine.search(\"car\")\n",
    "\n",
    "# Print search results\n",
    "for result in results:\n",
    "    print(result['_source'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06df06d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
